{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import cm, colors\n",
    "from sklearn.datasets import fetch_olivetti_faces"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-07-10T07:18:40.325922Z",
     "iopub.execute_input": "2023-07-10T07:18:40.326481Z",
     "iopub.status.idle": "2023-07-10T07:18:40.858632Z",
     "shell.execute_reply.started": "2023-07-10T07:18:40.326447Z",
     "shell.execute_reply": "2023-07-10T07:18:40.856226Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-07-11T05:09:09.027133300Z",
     "start_time": "2023-07-11T05:09:02.444788200Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Noise Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FGSM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def fgsm_attack(image, epsilon, gradient):\n",
    "#     # Calculate the sign of the gradient\n",
    "#     signed_grad = tf.sign(gradient)\n",
    "    \n",
    "#     # Create the perturbed image by adding the scaled signed gradient to the original image\n",
    "#     perturbed_image = image + epsilon * signed_grad\n",
    "    \n",
    "#     # Clip the pixel values of the perturbed image to the valid range [0, 1]\n",
    "#     perturbed_image = tf.clip_by_value(perturbed_image, 0, 1)\n",
    "    \n",
    "#     return perturbed_image\n",
    "\n",
    "# def apply_fgsm_attack_to_folder(folder_path, output_path, epsilon):\n",
    "#     # Load or build your pre-trained model\n",
    "#     model = tf.keras.applications.ResNet50(weights='imagenet')\n",
    "    \n",
    "#     # Create the output folder if it doesn't exist\n",
    "#     if not os.path.exists(output_path):\n",
    "#         os.makedirs(output_path)\n",
    "    \n",
    "#     # Iterate over the images in the folder\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         image_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "#         # Load and preprocess the image\n",
    "#         image = tf.io.read_file(image_path)\n",
    "#         image = tf.image.decode_jpeg(image, channels=3)\n",
    "#         image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "#         image = tf.image.resize(image, (224, 224))  # Resize the image to the desired dimensions\n",
    "        \n",
    "#         # Create a tensor variable from the image\n",
    "#         image_tensor = tf.Variable(np.expand_dims(image, axis=0))\n",
    "        \n",
    "#         # Compute the gradient of the loss with respect to the input image\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             tape.watch(image_tensor)\n",
    "#             predictions = model(image_tensor)\n",
    "#             top_prediction = tf.argmax(predictions, axis=1)\n",
    "#             loss = tf.keras.losses.categorical_crossentropy(tf.one_hot(top_prediction, 1000), predictions)\n",
    "        \n",
    "#         gradients = tape.gradient(loss, image_tensor)\n",
    "        \n",
    "#         # Generate the adversarial example using FGSM\n",
    "#         perturbed_image = fgsm_attack(image_tensor, epsilon, gradients)\n",
    "        \n",
    "#         # Get the perturbed image as a NumPy array\n",
    "#         perturbed_image = np.squeeze(perturbed_image.numpy(), axis=0)\n",
    "        \n",
    "#         # Save the perturbed image to the output folder\n",
    "#         output_filename = os.path.join(output_path, filename)\n",
    "#         plt.imsave(output_filename, perturbed_image)\n",
    "#         print(f\"Saved perturbed image: {output_filename}\")\n"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-07-11T05:09:22.784293800Z",
     "start_time": "2023-07-11T05:09:22.690295400Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gaussian"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def add_gaussian_noise(image, mean, std_dev, epsilon):\n",
    "    noise = np.random.normal(mean, std_dev, image.shape)\n",
    "    noisy_image = np.clip(image + epsilon * noise, 0, 255).astype(np.uint8)\n",
    "    return noisy_image\n",
    "\n",
    "def gaussian_folder(input_path, output_path, epsilon):\n",
    "    # Specify the mean, standard deviation, and epsilon value\n",
    "    mean = 0\n",
    "    std_dev = 30\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    # Iterate over the images in the folder\n",
    "    for filename in os.listdir(input_path):\n",
    "        image_path = os.path.join(input_path, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        # Add Gaussian noise to the image with the adjusted intensity\n",
    "        noisy_image = add_gaussian_noise(image, mean, std_dev, epsilon)\n",
    "        noisy_path = os.path.join(output_path, filename)\n",
    "        cv2.imwrite(noisy_path, noisy_image)\n",
    "    \n",
    "    print(\"Done adding Gaussian mask to image.\")\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-10T07:19:01.021156Z",
     "iopub.execute_input": "2023-07-10T07:19:01.022032Z",
     "iopub.status.idle": "2023-07-10T07:19:01.182025Z",
     "shell.execute_reply.started": "2023-07-10T07:19:01.021989Z",
     "shell.execute_reply": "2023-07-10T07:19:01.181083Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-07-11T05:09:30.931920400Z",
     "start_time": "2023-07-11T05:09:29.643810100Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'cv2'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "input_path = '/kaggle/input/human-faces-object-detection/images'\n",
    "output_path = '/kaggle/working/Gaussian'\n",
    "epsilon = .5\n",
    "\n",
    "gaussian_folder(input_path, output_path, epsilon)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-10T07:19:05.454442Z",
     "iopub.execute_input": "2023-07-10T07:19:05.455005Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Robust PCA denoise"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import cupy as cp\n",
    "\n",
    "class Robust_PCA:\n",
    "\n",
    "    def __init__(self, imax=1000, fthres=1e-6, verbose=False):\n",
    "        self.imax = imax  # Maximum number of iterations\n",
    "        self.fthres = fthres  # Convergence threshold\n",
    "        self.verbose = verbose  # Print progress during iterations\n",
    "\n",
    "    def shrink(self, X, tau):\n",
    "        Y = cp.abs(X) - tau\n",
    "        return cp.sign(X) * cp.maximum(Y, cp.zeros_like(Y))\n",
    "\n",
    "    def SVT(self, X, tau):\n",
    "        U, S, VT = cp.linalg.svd(X, full_matrices=0)\n",
    "        out = U @ cp.diag(self.shrink(S, tau)) @ VT\n",
    "        return out\n",
    "\n",
    "    def fit(self, X):\n",
    "        m, n = X.shape  # Store matrix dimensions\n",
    "        mu = m * n / (4 * cp.sum(cp.abs(X.reshape(-1))))  # Calculate mu\n",
    "        lambd = 1 / cp.sqrt(cp.max(cp.asarray([m, n])))  # Calculate lambda\n",
    "        thres = self.fthres * cp.linalg.norm(X)  # Determine threshold (tolerance)\n",
    "\n",
    "        # Initialize matrices L, S, and Y\n",
    "        L = cp.zeros_like(X)\n",
    "        S = cp.zeros_like(X)\n",
    "        Y = cp.zeros_like(X)\n",
    "\n",
    "        count = 0\n",
    "        # Continue iterating while X â‰ˆ L + S is above threshold or maximum number of iterations has been reached\n",
    "        while (cp.linalg.norm(X - L - S) > thres) and (count < self.imax):\n",
    "            if self.verbose:\n",
    "                print(f'{cp.round(thres / cp.linalg.norm(X - L - S), 3)}% ...   ({count}/{self.imax})')\n",
    "\n",
    "            # Update L, S, and Y\n",
    "            L = self.SVT(X - S + 1 / mu * Y, 1 / mu)\n",
    "            S = self.shrink(X - L + 1 / mu * Y, lambd / mu)\n",
    "            Y = Y + mu * (X - L - S)\n",
    "            count += 1\n",
    "\n",
    "        # Print number of iterations required\n",
    "        if self.verbose:\n",
    "            if count == self.imax:\n",
    "                print(f'No convergence after {count} iterations')\n",
    "            else:\n",
    "                print(f'Solution found after {count} iterations')\n",
    "\n",
    "        self.L_ = L\n",
    "        self.S_ = S\n",
    "\n",
    "        return self\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "list_img = os.listdir('/kaggle/working/Gaussian')\n",
    "# list_img\n",
    "num_images = len(list_img)\n",
    "image_size = 128  # Set the desired size for the flattened images\n",
    "\n",
    "r_matrix = np.zeros((num_images, image_size * image_size), dtype=np.uint8)\n",
    "g_matrix = np.zeros((num_images, image_size * image_size), dtype=np.uint8)\n",
    "b_matrix = np.zeros((num_images, image_size * image_size), dtype=np.uint8)\n",
    "\n",
    "img_org_size = []\n",
    "for i, img in enumerate(list_img):\n",
    "    img_path = os.path.join('/kaggle/working/Gaussian', img)\n",
    "    img = cv2.imread(img_path)\n",
    "    height, width, _ = img.shape\n",
    "    img_org_size.append((height, width))\n",
    "    \n",
    "    b, g, r = cv2.split(img)   \n",
    "    b = cv2.resize(b, (image_size, image_size))\n",
    "    flattened_b = b.flatten()\n",
    "    b_matrix[i] = flattened_b\n",
    "    g = cv2.resize(g, (image_size, image_size))\n",
    "    flattened_g = g.flatten()\n",
    "    g_matrix[i] = flattened_g\n",
    "    r = cv2.resize(r, (image_size, image_size))\n",
    "    flattened_r = r.flatten()\n",
    "    r_matrix[i] = flattened_r\n",
    "#     print(img.shape)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "r_matrix = r_matrix / np.max(r_matrix)\n",
    "g_matrix = g_matrix / np.max(g_matrix)\n",
    "b_matrix = b_matrix / np.max(b_matrix)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Perform robust PCA on the digits-matrix\n",
    "# Example usage\n",
    "noise_b_gpu, noise_g_gpu, noise_r_gpu = cp.asarray(b_matrix), cp.asarray(g_matrix), cp.asarray(r_matrix)\n",
    "\n",
    "rpcadm = Robust_PCA(imax=300, fthres=1e-7, verbose=True)\n",
    "\n",
    "print('Computing Sparse PCA for blue matrix...')\n",
    "rpcadm.fit(noise_b_gpu)\n",
    "L_b, S_b = cp.asnumpy(rpcadm.L_), cp.asnumpy(rpcadm.S_)\n",
    "print('Done. Computing Sparse PCA for green matrix...')\n",
    "\n",
    "rpcadm.fit(noise_g_gpu)\n",
    "L_g, S_g = cp.asnumpy(rpcadm.L_), cp.asnumpy(rpcadm.S_)\n",
    "print('Done. Computing Sparse PCA for red matrix...')\n",
    "\n",
    "rpcadm.fit(noise_r_gpu)# Provide your input data matrix X\n",
    "L_r, S_r = cp.asnumpy(rpcadm.L_), cp.asnumpy(rpcadm.S_)# Obtained low-rank component\n",
    "print('Done')\n",
    "  # Obtained sparse component"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fig, axs = plt.subplots(5, 3, figsize = (7, 12))\n",
    "# image_size = 32\n",
    "\n",
    "for k in range(5):\n",
    "    red_image = r_matrix[k].reshape(image_size,image_size)\n",
    "    green_image = g_matrix[k].reshape(image_size ,image_size)\n",
    "    blue_image = b_matrix[k].reshape(image_size, image_size)\n",
    "    # Stack the individual color channels to create an RGB image\n",
    "    rgb_image = np.stack((red_image, green_image, blue_image), axis=2)\n",
    "    rgb_image = cv2.resize(rgb_image, (img_org_size[k][0], img_org_size[k][1]))\n",
    "    rgb_image_scaled = (rgb_image * 255).astype(np.uint8)\n",
    "    axs[k, 0].imshow(rgb_image_scaled)\n",
    "    \n",
    "    noise_red = S_r[k].reshape(image_size,image_size)\n",
    "    noise_green = S_g[k].reshape(image_size ,image_size)\n",
    "    noise_blue = S_b[k].reshape(image_size, image_size)\n",
    "    # Stack the individual color channels to create an RGB image\n",
    "    rgb_noise_image = np.stack((noise_red, noise_green, noise_blue), axis=2)\n",
    "    rgb_noise_image = cv2.resize(rgb_noise_image, (img_org_size[k][0], img_org_size[k][1]))\n",
    "    rgb_noise_image_scaled = (rgb_noise_image * 255).astype(np.uint8)\n",
    "    axs[k, 1].imshow(rgb_noise_image_scaled)\n",
    "    \n",
    "    Lr_image = L_r[k].reshape(image_size,image_size)\n",
    "    Lg_image = L_g[k].reshape(image_size ,image_size)\n",
    "    Lb_image = L_b[k].reshape(image_size, image_size)\n",
    "    # Stack the individual color channels to create an RGB image\n",
    "    L_image = np.stack((Lr_image, Lg_image, Lb_image), axis=2)\n",
    "    L_image = cv2.resize(L_image, (img_org_size[k][0], img_org_size[k][1]))\n",
    "    L_image_scaled = (L_image * 255).astype(np.uint8)\n",
    "    axs[k, 2].imshow(L_image_scaled)\n",
    "\n",
    "    for i in range(3):\n",
    "        axs[k,i].set_xticks([])\n",
    "        axs[k,i].set_yticks([])\n",
    "        \n",
    "    if k == 0:\n",
    "        axs[k,0].set_title('Noise image', size = 6)\n",
    "        axs[k,1].set_title('Noise matrix', size = 6)\n",
    "        axs[k,2].set_title('Low resolution image', size = 6)\n",
    "#     axs[i,1].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig('Test.jpeg', dpi = 400)\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
