{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-16T04:35:18.949596Z","iopub.execute_input":"2023-07-16T04:35:18.950464Z","iopub.status.idle":"2023-07-16T04:35:23.063454Z","shell.execute_reply.started":"2023-07-16T04:35:18.950428Z","shell.execute_reply":"2023-07-16T04:35:23.062341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5.git","metadata":{"execution":{"iopub.status.busy":"2023-07-16T06:43:24.168625Z","iopub.execute_input":"2023-07-16T06:43:24.169057Z","iopub.status.idle":"2023-07-16T06:43:26.906945Z","shell.execute_reply.started":"2023-07-16T06:43:24.169022Z","shell.execute_reply":"2023-07-16T06:43:26.905865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd yolov5","metadata":{"execution":{"iopub.status.busy":"2023-07-16T06:43:26.908833Z","iopub.execute_input":"2023-07-16T06:43:26.909756Z","iopub.status.idle":"2023-07-16T06:43:26.915821Z","shell.execute_reply.started":"2023-07-16T06:43:26.909721Z","shell.execute_reply":"2023-07-16T06:43:26.914697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -r requirements.txt\n!pip install torchinfo","metadata":{"execution":{"iopub.status.busy":"2023-07-16T06:43:26.917380Z","iopub.execute_input":"2023-07-16T06:43:26.917725Z","iopub.status.idle":"2023-07-16T06:44:10.580966Z","shell.execute_reply.started":"2023-07-16T06:43:26.917697Z","shell.execute_reply":"2023-07-16T06:44:10.579645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from utils.dataloaders import create_dataloader\nfrom utils.general import (LOGGER, TQDM_BAR_FORMAT, Profile, check_dataset, check_img_size, check_requirements,\n                           check_yaml, coco80_to_coco91_class, colorstr, increment_path, non_max_suppression,\n                           print_args, scale_boxes, xywh2xyxy, xyxy2xywh)\nimport cv2\nimport matplotlib.pyplot as plt\nfrom models.yolo import Model\nimport torch\nimport yaml\nfrom utils.general import (LOGGER, TQDM_BAR_FORMAT, check_amp, check_dataset, check_file, check_git_info,\n                           check_git_status, check_img_size, check_requirements, check_suffix, check_yaml, colorstr,\n                           get_latest_run, increment_path, init_seeds, intersect_dicts, labels_to_class_weights,\n                           labels_to_image_weights, methods, one_cycle, print_args, print_mutation, strip_optimizer,\n                           yaml_save)\nimport torchinfo\nfrom PIL import Image\nfrom utils.loss import ComputeLoss\nfrom utils.metrics import ConfusionMatrix, ap_per_class, box_iou\nfrom utils.general import (LOGGER, TQDM_BAR_FORMAT, Profile, check_dataset, check_img_size, check_requirements,\n                           check_yaml, coco80_to_coco91_class, colorstr, increment_path, non_max_suppression,\n                           print_args, scale_boxes, xywh2xyxy, xyxy2xywh)\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-07-16T07:21:02.297537Z","iopub.execute_input":"2023-07-16T07:21:02.297975Z","iopub.status.idle":"2023-07-16T07:21:02.307961Z","shell.execute_reply.started":"2023-07-16T07:21:02.297937Z","shell.execute_reply":"2023-07-16T07:21:02.306711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader, dataset = create_dataloader('/kaggle/input/congapro/content/Yolo_1/images/train',\n                                            640,\n                                            1,\n                                            32,\n                                            True,\n                                            pad = 0,\n                                            rect = True,\n                                            workers = 2,\n                                            prefix = colorstr(f'train'))","metadata":{"execution":{"iopub.status.busy":"2023-07-16T06:44:19.203650Z","iopub.execute_input":"2023-07-16T06:44:19.204234Z","iopub.status.idle":"2023-07-16T06:44:27.633598Z","shell.execute_reply.started":"2023-07-16T06:44:19.204201Z","shell.execute_reply":"2023-07-16T06:44:27.632404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (data, target, paths, _) in train_loader:\n    original_data = data.squeeze().permute(1, 2, 0).numpy()\n    plt.imshow(original_data)\n    print(original_data.shape)\n    print(paths)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-07-16T06:44:27.638185Z","iopub.execute_input":"2023-07-16T06:44:27.638641Z","iopub.status.idle":"2023-07-16T06:44:27.929288Z","shell.execute_reply.started":"2023-07-16T06:44:27.638603Z","shell.execute_reply":"2023-07-16T06:44:27.928352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/yolov5/data/hyps/hyp.scratch-low.yaml', errors = 'ignore') as f:\n  hyp = yaml.safe_load(f)\ndevice = 'cuda'\nweights = '/kaggle/input/model/best.pt'","metadata":{"execution":{"iopub.status.busy":"2023-07-16T06:44:27.931014Z","iopub.execute_input":"2023-07-16T06:44:27.931470Z","iopub.status.idle":"2023-07-16T06:44:27.947193Z","shell.execute_reply.started":"2023-07-16T06:44:27.931413Z","shell.execute_reply":"2023-07-16T06:44:27.946166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ckpt = torch.load(weights, map_location='cpu')  # load checkpoint to CPU to avoid CUDA memory leak\nmodel = Model(ckpt['model'].yaml, ch=3, nc=1, anchors=3).to(device)  # create\nexclude = ['anchor'] if (hyp.get('anchors')) and not resume else []  # exclude keys\ncsd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\ncsd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect\nmodel.load_state_dict(csd, strict=False)  # load\nLOGGER.info(f'Transferred {len(csd)}/{len(model.state_dict())} items from {weights}')  # report","metadata":{"execution":{"iopub.status.busy":"2023-07-16T06:44:27.949897Z","iopub.execute_input":"2023-07-16T06:44:27.950237Z","iopub.status.idle":"2023-07-16T06:44:28.744652Z","shell.execute_reply.started":"2023-07-16T06:44:27.950209Z","shell.execute_reply":"2023-07-16T06:44:28.743628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FGSM attack code\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image\n\n# restores the tensors to their original scale\ndef denorm(batch, mean=[0.1307], std=[0.3081]):\n    \"\"\"\n    Convert a batch of tensors to their original scale.\n\n    Args:\n        batch (torch.Tensor): Batch of normalized tensors.\n        mean (torch.Tensor or list): Mean used for normalization.\n        std (torch.Tensor or list): Standard deviation used for normalization.\n\n    Returns:\n        torch.Tensor: batch of tensors without normalization applied to them.\n    \"\"\"\n    if isinstance(mean, list):\n        mean = torch.tensor(mean).to(device)\n    if isinstance(std, list):\n        std = torch.tensor(std).to(device)\n\n    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T07:23:30.953216Z","iopub.execute_input":"2023-07-16T07:23:30.953732Z","iopub.status.idle":"2023-07-16T07:23:30.967313Z","shell.execute_reply.started":"2023-07-16T07:23:30.953687Z","shell.execute_reply":"2023-07-16T07:23:30.965371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from google.colab.patches import cv2_imshow\nimport matplotlib.pyplot as plt\ndef test( model, device, test_loader, epsilon ):\n    pbar = enumerate(test_loader)\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n    model.hyp = hyp\n    compute_loss = ComputeLoss(model)\n\n    # Loop over all examples in test set\n    for i, (data, target, paths, _) in pbar:\n#         print('/kaggle/working/data_fgsm/' + paths[0].split('/')[-1])\n        original_data = data.squeeze().permute(1, 2, 0).numpy()\n#         plt.imshow(original_data)\n        # img = Image.fromarray(original_data, 'RGB')\n        # img.save('/content/lucdau.png')\n#         tmp = cv2.imread(paths[0])\n        # print(tmp)\n        # print(original_data)\n#         cv2_imshow(tmp)\n#         print('ðŸ’©' * 50)\n        # cv2_imshow(original_data)\n\n        # Send the data and label to the device\n        data = data.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0\n        # data = data.permute(0, 2, 3, 1).type(torch.float)\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n        # data.retain_grad()\n        target = target.to(device)\n        # Forward pass the data through the model\n        output = model(data)\n#         print(len(output))\n#         output = torch.unsqueeze(output, 0)\n#         init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n\n        # If the initial prediction is wrong, don't bother attacking, just move on\n#         if init_pred.item() != target.item():\n#             continue\n        # print(output.dtype, target.dtype)\n        # Calculate the loss\n        loss, loss_items = compute_loss(output, target.to(device))\n        # print('ðŸ’©' * 50)\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n#         print(loss)\n        # print(output.pandas().xyxy[0])\n        # for si, pred in enumerate(output):\n        #   print(si)\n        #   labels = target[target[:, 0] == si, 1:]\n        #   nl, npr = labels.shape[0], pred.shape[0]  # number of labels, predictions\n        #   # path, shape = Path(paths[si]), shapes[si][0]\n        #   print(nl, npr)\n        #   print(labels.shape)\n        #   print(pred.shape)\n        #   predn = pred.clone()\n        #   tbox = xywh2xyxy(labels[:, 1:5])\n        #   print(labels[:, 1:].shape, predn[:, :4].shape)\n        #   iou = box_iou(labels[:, 1:], predn[:, :4])\n        #   print(labels[:, 1:].shape, predn[:, :4].shape)\n        #   print(iou)\n\n        # Collect ``datagrad``\n        data_grad = data.grad.data\n\n        # Restore the data to its original scale\n#         data_denorm = denorm(data)\n        data_denorm = torch.clone(data).to(device)\n        data_denorm_numpy = data_denorm.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy() * 255\n        cv2.imwrite('/kaggle/working/data_fgsm/images/train/' + paths[0].split('/')[-1], cv2.cvtColor(data_denorm_numpy, cv2.COLOR_RGB2BGR))\n        # cv2_imshow(cv2.cvtColor(data_denorm_numpy, cv2.COLOR_RGB2BGR))\n        # print(data_denorm_numpy.shape, data_denorm_numpy)\n        # img = Image.fromarray(data_denorm_numpy)\n        # img.show()\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data_denorm, 0.05, data_grad)\n        perturbed_data_numpy = perturbed_data.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy() * 255\n#         plt.imshow(cv2.cvtColor(perturbed_data_numpy, cv2.COLOR_RGB2BGR))\n        cv2.imwrite('/kaggle/working/data_fgsm/noised_images/train/' + paths[0].split('/')[-1], cv2.cvtColor(perturbed_data_numpy, cv2.COLOR_RGB2BGR))\n        # Reapply normalization\n        # perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n\n        # Re-classify the perturbed image\n        # output = model(perturbed_data)\n        # # print(type(output), target)\n\n\n        # # Check for success\n        # final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        # if final_pred.item() == target.item():\n        #     correct += 1\n        #     # Special case for saving 0 epsilon examples\n        #     if epsilon == 0 and len(adv_examples) < 5:\n        #         adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n        #         adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n        # else:\n        #     # Save some adv examples for visualization later\n        #     if len(adv_examples) < 5:\n        #         adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n        #         adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n\n    # Calculate final accuracy for this epsilon\n    # final_acc = correct/float(len(test_loader))\n    # print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}\")\n\n    # # Return the accuracy and an adversarial example\n    # return final_acc, adv_examples\n    \n    return 1, 1","metadata":{"execution":{"iopub.status.busy":"2023-07-16T07:23:31.589217Z","iopub.execute_input":"2023-07-16T07:23:31.589883Z","iopub.status.idle":"2023-07-16T07:23:31.610564Z","shell.execute_reply.started":"2023-07-16T07:23:31.589832Z","shell.execute_reply":"2023-07-16T07:23:31.609110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epsilons = [.1]\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nos.makedirs('/kaggle/working/data_fgsm/',exist_ok = True)\nos.makedirs('/kaggle/working/data_fgsm/images/',exist_ok = True)\nos.makedirs('/kaggle/working/data_fgsm/images/train',exist_ok = True)\nos.makedirs('/kaggle/working/data_fgsm/images/val',exist_ok = True)\nos.makedirs('/kaggle/working/data_fgsm/noised_images/',exist_ok = True)\nos.makedirs('/kaggle/working/data_fgsm/noised_images/train',exist_ok = True)\nos.makedirs('/kaggle/working/data_fgsm/noised_images/val',exist_ok = True)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T07:25:04.265099Z","iopub.execute_input":"2023-07-16T07:25:04.265733Z","iopub.status.idle":"2023-07-16T07:25:04.274174Z","shell.execute_reply.started":"2023-07-16T07:25:04.265682Z","shell.execute_reply":"2023-07-16T07:25:04.272561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    model.to(device)\n    acc, ex = test(model, device, train_loader, eps)\n    accuracies.append(acc)\n    examples.append(ex)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T07:25:04.482327Z","iopub.execute_input":"2023-07-16T07:25:04.483059Z","iopub.status.idle":"2023-07-16T07:25:11.112785Z","shell.execute_reply.started":"2023-07-16T07:25:04.483020Z","shell.execute_reply":"2023-07-16T07:25:11.111382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader, dataset = create_dataloader('/kaggle/input/congapro/content/Yolo_1/images/val',\n                                            640,\n                                            1,\n                                            32,\n                                            True,\n                                            pad = 0,\n                                            rect = True,\n                                            workers = 2,\n                                            prefix = colorstr(f'train'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from google.colab.patches import cv2_imshow\nimport matplotlib.pyplot as plt\ndef test( model, device, test_loader, epsilon ):\n    pbar = enumerate(test_loader)\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n    model.hyp = hyp\n    compute_loss = ComputeLoss(model)\n\n    # Loop over all examples in test set\n    for i, (data, target, paths, _) in pbar:\n#         print('/kaggle/working/data_fgsm/' + paths[0].split('/')[-1])\n        original_data = data.squeeze().permute(1, 2, 0).numpy()\n#         plt.imshow(original_data)\n        # img = Image.fromarray(original_data, 'RGB')\n        # img.save('/content/lucdau.png')\n#         tmp = cv2.imread(paths[0])\n        # print(tmp)\n        # print(original_data)\n#         cv2_imshow(tmp)\n#         print('ðŸ’©' * 50)\n        # cv2_imshow(original_data)\n\n        # Send the data and label to the device\n        data = data.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0\n        # data = data.permute(0, 2, 3, 1).type(torch.float)\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n        # data.retain_grad()\n        target = target.to(device)\n        # Forward pass the data through the model\n        output = model(data)\n#         print(len(output))\n#         output = torch.unsqueeze(output, 0)\n#         init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n\n        # If the initial prediction is wrong, don't bother attacking, just move on\n#         if init_pred.item() != target.item():\n#             continue\n        # print(output.dtype, target.dtype)\n        # Calculate the loss\n        loss, loss_items = compute_loss(output, target.to(device))\n        # print('ðŸ’©' * 50)\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n#         print(loss)\n        # print(output.pandas().xyxy[0])\n        # for si, pred in enumerate(output):\n        #   print(si)\n        #   labels = target[target[:, 0] == si, 1:]\n        #   nl, npr = labels.shape[0], pred.shape[0]  # number of labels, predictions\n        #   # path, shape = Path(paths[si]), shapes[si][0]\n        #   print(nl, npr)\n        #   print(labels.shape)\n        #   print(pred.shape)\n        #   predn = pred.clone()\n        #   tbox = xywh2xyxy(labels[:, 1:5])\n        #   print(labels[:, 1:].shape, predn[:, :4].shape)\n        #   iou = box_iou(labels[:, 1:], predn[:, :4])\n        #   print(labels[:, 1:].shape, predn[:, :4].shape)\n        #   print(iou)\n\n        # Collect ``datagrad``\n        data_grad = data.grad.data\n\n        # Restore the data to its original scale\n#         data_denorm = denorm(data)\n        data_denorm = torch.clone(data).to(device)\n        data_denorm_numpy = data_denorm.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy() * 255\n        cv2.imwrite('/kaggle/working/data_fgsm/images/val/' + paths[0].split('/')[-1], cv2.cvtColor(data_denorm_numpy, cv2.COLOR_RGB2BGR))\n        # cv2_imshow(cv2.cvtColor(data_denorm_numpy, cv2.COLOR_RGB2BGR))\n        # print(data_denorm_numpy.shape, data_denorm_numpy)\n        # img = Image.fromarray(data_denorm_numpy)\n        # img.show()\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data_denorm, 0.05, data_grad)\n        perturbed_data_numpy = perturbed_data.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy() * 255\n#         plt.imshow(cv2.cvtColor(perturbed_data_numpy, cv2.COLOR_RGB2BGR))\n        cv2.imwrite('/kaggle/working/data_fgsm/noised_images/val/' + paths[0].split('/')[-1], cv2.cvtColor(perturbed_data_numpy, cv2.COLOR_RGB2BGR))\n        # Reapply normalization\n        # perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n\n        # Re-classify the perturbed image\n        # output = model(perturbed_data)\n        # # print(type(output), target)\n\n\n        # # Check for success\n        # final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        # if final_pred.item() == target.item():\n        #     correct += 1\n        #     # Special case for saving 0 epsilon examples\n        #     if epsilon == 0 and len(adv_examples) < 5:\n        #         adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n        #         adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n        # else:\n        #     # Save some adv examples for visualization later\n        #     if len(adv_examples) < 5:\n        #         adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n        #         adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n\n    # Calculate final accuracy for this epsilon\n    # final_acc = correct/float(len(test_loader))\n    # print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}\")\n\n    # # Return the accuracy and an adversarial example\n    # return final_acc, adv_examples\n    \n    return 1, 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    model.to(device)\n    acc, ex = test(model, device, train_loader, eps)\n    accuracies.append(acc)\n    examples.append(ex)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cp -R /kaggle/input/congapro/content/Yolo_1/labels /kaggle/working/data_fgsm/labels","metadata":{"execution":{"iopub.status.busy":"2023-07-16T07:21:48.847927Z","iopub.execute_input":"2023-07-16T07:21:48.848423Z","iopub.status.idle":"2023-07-16T07:21:53.444106Z","shell.execute_reply.started":"2023-07-16T07:21:48.848376Z","shell.execute_reply":"2023-07-16T07:21:53.442267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\nimport os\nfrom IPython.display import FileLink\n\ndef zip_dir(directory = os.curdir, file_name = 'directory.zip'):\n    \"\"\"\n    zip all the files in a directory\n    \n    Parameters\n    _____\n    directory: str\n        directory needs to be zipped, defualt is current working directory\n        \n    file_name: str\n        the name of the zipped file (including .zip), default is 'directory.zip'\n        \n    Returns\n    _____\n    Creates a hyperlink, which can be used to download the zip file)\n    \"\"\"\n    os.chdir(directory)\n    zip_ref = zipfile.ZipFile(file_name, mode='w')\n    for folder, _, files in os.walk(directory):\n        for file in files:\n            if file_name in file:\n                pass\n            else:\n                zip_ref.write(os.path.join(folder, file))\n\n    return FileLink(file_name)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T07:30:34.468163Z","iopub.execute_input":"2023-07-16T07:30:34.468632Z","iopub.status.idle":"2023-07-16T07:30:34.480185Z","shell.execute_reply.started":"2023-07-16T07:30:34.468599Z","shell.execute_reply":"2023-07-16T07:30:34.478919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd ..","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip_dir('/kaggle/working/data_fgsm',)","metadata":{},"execution_count":null,"outputs":[]}]}